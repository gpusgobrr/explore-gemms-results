
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CUDA GEMM Benchmark Results</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            padding: 20px;
        }

        .container {
            max-width: 1400px;
            margin: 0 auto;
            background: white;
            border-radius: 20px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
            overflow: hidden;
        }

        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }

        h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
        }

        .subtitle {
            font-size: 1.1em;
            opacity: 0.9;
        }

        .info-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            padding: 30px;
            background: #f8f9fa;
            border-bottom: 3px solid #e9ecef;
        }

        .info-card {
            background: white;
            padding: 20px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }

        .info-card h3 {
            color: #667eea;
            margin-bottom: 10px;
            font-size: 0.9em;
            text-transform: uppercase;
            letter-spacing: 1px;
        }

        .info-card p {
            color: #333;
            font-size: 1.3em;
            font-weight: bold;
        }

        .chart-container {
            padding: 30px;
        }

        .chart-container iframe {
            width: 100%;
            height: 950px;
            border: none;
            border-radius: 10px;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 30px;
            max-width: calc(100% - 60px);
        }

        th, td {
            padding: 15px;
            text-align: left;
            border-bottom: 1px solid #e9ecef;
        }

        th {
            background: #667eea;
            color: white;
            font-weight: 600;
            text-transform: uppercase;
            font-size: 0.85em;
            letter-spacing: 1px;
        }

        tr:hover {
            background: #f8f9fa;
        }

        .kernel-badge {
            display: inline-block;
            padding: 5px 12px;
            border-radius: 20px;
            font-size: 0.85em;
            font-weight: 600;
        }

        .badge-pytorch {
            background: #e3f2fd;
            color: #1976d2;
        }

        .badge-pytorch-jit {
            background: #f3e5f5;
            color: #7b1fa2;
        }

        .badge-naive {
            background: #ffebee;
            color: #c62828;
        }

        .badge-coalesced {
            background: #e8f5e9;
            color: #2e7d32;
        }

        .badge-shared {
            background: #f3e5f5;
            color: #7b1fa2;
        }

        .badge-blocktiling {
            background: #fff3e0;
            color: #e65100;
        }

        .badge-blocktiling2d {
            background: #e0f7fa;
            color: #006064;
        }

        .badge-vectorize {
            background: #fce4ec;
            color: #c2185b;
        }

        .badge-warptiling {
            background: #fff9c4;
            color: #f57f17;
        }

        .badge-tensorcore {
            background: #e0f2f1;
            color: #00695c;
        }

        .badge-cutlass {
            background: #ffe0b2;
            color: #e65100;
        }

        .speedup {
            font-weight: bold;
        }

        .speedup.faster {
            color: #2e7d32;
        }

        .speedup.slower {
            color: #c62828;
        }

        footer {
            text-align: center;
            padding: 20px;
            background: #f8f9fa;
            color: #666;
            font-size: 0.9em;
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>üöÄ CUDA GEMM Kernel Benchmarks (BFP16)</h1>
            <p class="subtitle">Performance Comparison: PyTorch vs Custom CUDA Kernels</p>
        </header>

        <div class="info-grid">
            <div class="info-card">
                <h3>üñ•Ô∏è GPU</h3>
                <p>NVIDIA H100 80GB HBM3</p>
            </div>
            <div class="info-card">
                <h3>üìä Data Type</h3>
                <p>BFP16</p>
            </div>
            <div class="info-card">
                <h3>‚è∞ Timestamp</h3>
                <p>2025-12-30 11:01:35</p>
            </div>
            <div class="info-card">
                <h3>üî¢ Test Sizes</h3>
                <p>12 configurations</p>
            </div>
            <div class="info-card">
                <h3>‚ö° Kernels Tested</h3>
                <p>2 kernels</p>
            </div>
        </div>

        <div class="chart-container">
            <iframe src="benchmark_results.html"></iframe>
        </div>

        <h2 style="padding: 30px 30px 0 30px; color: #333;">üìä Detailed Results</h2>
        <table>
            <thead>
                <tr>
                    <th>Matrix Size</th>
                    <th>Kernel</th>
                    <th>Avg Time (ms)</th>
                    <th>TFLOP16s</th>
                    <th>Bandwidth (GB/s)</th>
                    <th>Speedup</th>
                </tr>
            </thead>
            <tbody>

                <tr>
                    <td><strong>64√ó64</strong></td>
                    <td><span class="kernel-badge badge-pytorch">PyTorch</span></td>
                    <td>0.0253</td>
                    <td><strong>0.02</strong></td>
                    <td>0.97</td>
                    <td><span class="speedup ">baseline</span></td>
                </tr>

                <tr>
                    <td><strong>64√ó64</strong></td>
                    <td><span class="kernel-badge badge-cutlass">CUTLASS Hopper TMA Stream-K Const</span></td>
                    <td>0.0253</td>
                    <td><strong>0.02</strong></td>
                    <td>0.97</td>
                    <td><span class="speedup ">1.00√ó</span></td>
                </tr>

                <tr>
                    <td><strong>96√ó96</strong></td>
                    <td><span class="kernel-badge badge-pytorch">PyTorch</span></td>
                    <td>0.0248</td>
                    <td><strong>0.07</strong></td>
                    <td>2.23</td>
                    <td><span class="speedup ">baseline</span></td>
                </tr>

                <tr>
                    <td><strong>96√ó96</strong></td>
                    <td><span class="kernel-badge badge-cutlass">CUTLASS Hopper TMA Stream-K Const</span></td>
                    <td>0.0256</td>
                    <td><strong>0.07</strong></td>
                    <td>2.16</td>
                    <td><span class="speedup slower">0.97√ó</span></td>
                </tr>

                <tr>
                    <td><strong>128√ó128</strong></td>
                    <td><span class="kernel-badge badge-pytorch">PyTorch</span></td>
                    <td>0.0250</td>
                    <td><strong>0.17</strong></td>
                    <td>3.93</td>
                    <td><span class="speedup ">baseline</span></td>
                </tr>

                <tr>
                    <td><strong>128√ó128</strong></td>
                    <td><span class="kernel-badge badge-cutlass">CUTLASS Hopper TMA Stream-K Const</span></td>
                    <td>0.0257</td>
                    <td><strong>0.16</strong></td>
                    <td>3.83</td>
                    <td><span class="speedup slower">0.97√ó</span></td>
                </tr>

                <tr>
                    <td><strong>256√ó256</strong></td>
                    <td><span class="kernel-badge badge-pytorch">PyTorch</span></td>
                    <td>0.0248</td>
                    <td><strong>1.35</strong></td>
                    <td>15.86</td>
                    <td><span class="speedup ">baseline</span></td>
                </tr>

                <tr>
                    <td><strong>256√ó256</strong></td>
                    <td><span class="kernel-badge badge-cutlass">CUTLASS Hopper TMA Stream-K Const</span></td>
                    <td>0.0271</td>
                    <td><strong>1.24</strong></td>
                    <td>14.52</td>
                    <td><span class="speedup slower">0.92√ó</span></td>
                </tr>

                <tr>
                    <td><strong>512√ó512</strong></td>
                    <td><span class="kernel-badge badge-pytorch">PyTorch</span></td>
                    <td>0.0246</td>
                    <td><strong>10.89</strong></td>
                    <td>63.83</td>
                    <td><span class="speedup ">baseline</span></td>
                </tr>

                <tr>
                    <td><strong>512√ó512</strong></td>
                    <td><span class="kernel-badge badge-cutlass">CUTLASS Hopper TMA Stream-K Const</span></td>
                    <td>0.0297</td>
                    <td><strong>9.03</strong></td>
                    <td>52.91</td>
                    <td><span class="speedup slower">0.83√ó</span></td>
                </tr>

                <tr>
                    <td><strong>768√ó768</strong></td>
                    <td><span class="kernel-badge badge-pytorch">PyTorch</span></td>
                    <td>0.0252</td>
                    <td><strong>35.97</strong></td>
                    <td>140.52</td>
                    <td><span class="speedup ">baseline</span></td>
                </tr>

                <tr>
                    <td><strong>768√ó768</strong></td>
                    <td><span class="kernel-badge badge-cutlass">CUTLASS Hopper TMA Stream-K Const</span></td>
                    <td>0.1908</td>
                    <td><strong>4.75</strong></td>
                    <td>18.54</td>
                    <td><span class="speedup slower">0.13√ó</span></td>
                </tr>

                <tr>
                    <td><strong>1024√ó1024</strong></td>
                    <td><span class="kernel-badge badge-pytorch">PyTorch</span></td>
                    <td>0.0255</td>
                    <td><strong>84.31</strong></td>
                    <td>246.99</td>
                    <td><span class="speedup ">baseline</span></td>
                </tr>

                <tr>
                    <td><strong>1024√ó1024</strong></td>
                    <td><span class="kernel-badge badge-cutlass">CUTLASS Hopper TMA Stream-K Const</span></td>
                    <td>0.1960</td>
                    <td><strong>10.96</strong></td>
                    <td>32.10</td>
                    <td><span class="speedup slower">0.13√ó</span></td>
                </tr>

                <tr>
                    <td><strong>1536√ó1536</strong></td>
                    <td><span class="kernel-badge badge-pytorch">PyTorch</span></td>
                    <td>0.0294</td>
                    <td><strong>246.46</strong></td>
                    <td>481.36</td>
                    <td><span class="speedup ">baseline</span></td>
                </tr>

                <tr>
                    <td><strong>1536√ó1536</strong></td>
                    <td><span class="kernel-badge badge-cutlass">CUTLASS Hopper TMA Stream-K Const</span></td>
                    <td>0.2069</td>
                    <td><strong>35.03</strong></td>
                    <td>68.41</td>
                    <td><span class="speedup slower">0.14√ó</span></td>
                </tr>

                <tr>
                    <td><strong>2048√ó2048</strong></td>
                    <td><span class="kernel-badge badge-pytorch">PyTorch</span></td>
                    <td>0.0416</td>
                    <td><strong>413.30</strong></td>
                    <td>605.41</td>
                    <td><span class="speedup ">baseline</span></td>
                </tr>

                <tr>
                    <td><strong>2048√ó2048</strong></td>
                    <td><span class="kernel-badge badge-cutlass">CUTLASS Hopper TMA Stream-K Const</span></td>
                    <td>0.2257</td>
                    <td><strong>76.12</strong></td>
                    <td>111.50</td>
                    <td><span class="speedup slower">0.18√ó</span></td>
                </tr>

                <tr>
                    <td><strong>3072√ó3072</strong></td>
                    <td><span class="kernel-badge badge-pytorch">PyTorch</span></td>
                    <td>0.0924</td>
                    <td><strong>627.73</strong></td>
                    <td>613.02</td>
                    <td><span class="speedup ">baseline</span></td>
                </tr>

                <tr>
                    <td><strong>3072√ó3072</strong></td>
                    <td><span class="kernel-badge badge-cutlass">CUTLASS Hopper TMA Stream-K Const</span></td>
                    <td>0.3057</td>
                    <td><strong>189.69</strong></td>
                    <td>185.25</td>
                    <td><span class="speedup slower">0.30√ó</span></td>
                </tr>

                <tr>
                    <td><strong>4096√ó4096</strong></td>
                    <td><span class="kernel-badge badge-pytorch">PyTorch</span></td>
                    <td>0.1902</td>
                    <td><strong>722.51</strong></td>
                    <td>529.18</td>
                    <td><span class="speedup ">baseline</span></td>
                </tr>

                <tr>
                    <td><strong>4096√ó4096</strong></td>
                    <td><span class="kernel-badge badge-cutlass">CUTLASS Hopper TMA Stream-K Const</span></td>
                    <td>0.4434</td>
                    <td><strong>309.96</strong></td>
                    <td>227.02</td>
                    <td><span class="speedup slower">0.43√ó</span></td>
                </tr>

                <tr>
                    <td><strong>8192√ó8192</strong></td>
                    <td><span class="kernel-badge badge-pytorch">PyTorch</span></td>
                    <td>1.5809</td>
                    <td><strong>695.51</strong></td>
                    <td>254.70</td>
                    <td><span class="speedup ">baseline</span></td>
                </tr>

                <tr>
                    <td><strong>8192√ó8192</strong></td>
                    <td><span class="kernel-badge badge-cutlass">CUTLASS Hopper TMA Stream-K Const</span></td>
                    <td>2.0856</td>
                    <td><strong>527.20</strong></td>
                    <td>193.06</td>
                    <td><span class="speedup slower">0.76√ó</span></td>
                </tr>

            </tbody>
        </table>

        <footer>
            <p>Generated by CUDA GEMM Benchmark Suite ‚Ä¢ Powered by PyTorch & Plotly</p>
        </footer>
    </div>
</body>
</html>
